{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12. Scikit-Learn Machine Learning Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Required steps\n",
    "### Pre-process\n",
    "    - [X] Categorical to Numeric\n",
    "    - [X] Clean Numeric Data\n",
    "    - [X] Outlier Detection\n",
    "    - [X] Data Transformation\n",
    "    - [X] Data Normalization\n",
    "    - [X] Dimensionality Reduction\n",
    "    \n",
    "### Training steps - models\n",
    "    - [ ] XGBoost Regression\n",
    "    - [ ] Lasso Regression\n",
    "    - [ ] ANN Regression\n",
    "    - [ ] Random Forest Regression\n",
    "    - [ ] ElasticNet\n",
    "    - [ ] Bayesian Ridge Regression\n",
    "    - [ ] LassoLarsIC Regression\n",
    "    \n",
    "### Stacking\n",
    "    - [ ] Picking one of the traning models as Meta-model\n",
    "    - [ ] Use the rest for training the evaluation set and test on the test set.\n",
    "    - [ ] Use Meta-model to predict the test set based on the trained models results.\n",
    "    \n",
    "### Ensembling [?]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------\n",
    "## Implementation\n",
    "## Pre-process classes definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert Categorical columns to Numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json \n",
    "import category_encoders as ce\n",
    "# categorical missing value imputer\n",
    "class CatToNum(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self, dict_address, continuesVars, descreteVars_Ordinal, descreteVars_Nominal):\n",
    "        self.continuesVars = continuesVars\n",
    "        self.descreteVars_Ordinal = descreteVars_Ordinal\n",
    "        self.descreteVars_Nominal = descreteVars_Nominal\n",
    "        \n",
    "        fileName = dict_address\n",
    "        f = open(fileName,'r')\n",
    "        self.conversion_dict = json.loads(f.read())\n",
    "        \n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        # we need the fit statement to accomodate the sklearn pipeline\n",
    "        des_nom_DF = X[self.descreteVars_Nominal]\n",
    "        # Map Nominal Categorical data to Numerical\n",
    "        cat_nom_DF = des_nom_DF.fillna('NULL').astype(str)\n",
    "        self.ce_binary = ce.BinaryEncoder()\n",
    "        self.ce_binary.fit(cat_nom_DF)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        totalDF = X.copy()\n",
    "        conDF = X[self.continuesVars]\n",
    "        des_ord_DF = X[self.descreteVars_Ordinal]\n",
    "        des_nom_DF = X[self.descreteVars_Nominal]\n",
    "        \n",
    "        # Map Ordinal Categorical data to Numerical\n",
    "        cat_ord_DF_numerical = des_ord_DF.copy()\n",
    "        for feature in self.conversion_dict:\n",
    "            temp_dict = self.conversion_dict[feature]\n",
    "            if ('NA' in temp_dict): # Replace 'NA' with np.nan\n",
    "                temp_dict[np.nan] = temp_dict.pop('NA')\n",
    "            cat_ord_DF_numerical[feature] = des_ord_DF[feature].map(temp_dict)\n",
    "            \n",
    "        totalDF[self.descreteVars_Ordinal] = cat_ord_DF_numerical\n",
    "            \n",
    "        # Map Nominal Categorical data to Numerical\n",
    "        cat_nom_DF = des_nom_DF.fillna('NULL').astype(str)\n",
    "        cat_nom_DF_numerical = self.ce_binary.transform(cat_nom_DF)\n",
    "        totalDF = pd.concat([totalDF, cat_nom_DF_numerical], axis=1)\n",
    "        totalDF.drop(self.descreteVars_Nominal, axis=1, inplace=True)\n",
    "        cols_to_drop = [x for x in list(cat_nom_DF_numerical) if ('_0' in x)]\n",
    "        totalDF.drop(cols_to_drop, axis=1, inplace=True)\n",
    "\n",
    "        return totalDF\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean Numeric Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CleanNum(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self, measure='mean', drop_thresh=0.8):\n",
    "        self.measure = measure\n",
    "        self.drop_thresh = drop_thresh\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        train_df = X.copy().astype(float)\n",
    "        # Remove columns with dominance bigger than a threshold\n",
    "        self.drop_list = []\n",
    "        for feature in train_df:\n",
    "            col_df = train_df[feature]\n",
    "            count_nan = col_df.isnull().sum()\n",
    "            nan_ratio = count_nan/len(col_df)   \n",
    "            repeats = train_df.pivot_table(index=[feature], aggfunc='size').sort_values()\n",
    "            max_repeat_ratio = repeats.max()/len(col_df)\n",
    "            if (nan_ratio>self.drop_thresh or max_repeat_ratio>self.drop_thresh):\n",
    "                self.drop_list.append(feature)\n",
    "\n",
    "        # Replace null values with average (or mode) of the train columns\n",
    "        self.cols_average = train_df.mean(axis = 0)\n",
    "        self.cols_mode = train_df.mode(axis = 0)\n",
    "        \n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        totalDF = X.copy().astype(float)\n",
    "        \n",
    "        # Replace null values with average (or mode) of the train columns\n",
    "        for col in totalDF:\n",
    "            if (self.measure=='mean'):\n",
    "                totalDF[col] = totalDF[col].fillna(self.cols_average[col])\n",
    "            if (self.measure=='mode'):\n",
    "                totalDF[col] = totalDF[col].fillna(self.cols_mode[col])\n",
    "        \n",
    "        # Remove columns with dominance bigger than a threshold\n",
    "        totalDF.drop(self.drop_list, axis=1, inplace=True)\n",
    "\n",
    "        return totalDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outlier Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class OutlierDetection(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "        # Function to Detection Outlier on one-dimentional datasets.\n",
    "    def find_anomalies(self,data):\n",
    "        anomalies_idx = []\n",
    "        # Set upper and lower limit to 3 standard deviation\n",
    "        data_std = data.std()\n",
    "        data_mean = data.mean()\n",
    "        anomaly_cut_off = data_std * 3\n",
    "\n",
    "        lower_limit  = data_mean - anomaly_cut_off \n",
    "        upper_limit = data_mean + anomaly_cut_off\n",
    "        # Generate outliers\n",
    "        for idx in range(len(data)):\n",
    "            outlier = data[idx]\n",
    "            if outlier > upper_limit or outlier < lower_limit:\n",
    "                anomalies_idx.append(idx)\n",
    "        return anomalies_idx\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        train_df = X.copy()\n",
    "        outliers_full_list = []\n",
    "        for feature in train_df:\n",
    "            outliers_full_list = outliers_full_list + self.find_anomalies(train_df[feature].values)\n",
    "\n",
    "        self.outliers_unique = list(set(int(outliers_full_list)))\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        totalDF = X.copy()\n",
    "        totalDF.drop(self.outliers_unique, inplace=True)\n",
    "        return totalDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Transformation (Box-Cox transformation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.special import boxcox1p\n",
    "from scipy.stats import norm, skew\n",
    "\n",
    "class DataTransformation(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, X, y=None, thresh=0.75, lam=0.15):\n",
    "        self.lam = lam\n",
    "        train_df = X.copy()\n",
    "        # Check the skew of all numerical features\n",
    "#         print(type(skew(train_df)))\n",
    "        skewed_feats = train_df.apply(lambda x: skew(x.dropna())).sort_values(ascending=False)\n",
    "#         print(type(skewed_feats ))\n",
    "        skewness = pd.DataFrame({'Skew' :skewed_feats})\n",
    "        skewness = skewness[abs(skewness) > thresh]\n",
    "        self.skewed_features_idx_list = skewness.index\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        totalDF = X.copy()\n",
    "#         skewed_features = totalDF[self.skewed_features_list]\n",
    "        for feat_idx in self.skewed_features_idx_list:\n",
    "            #all_data[feat] += 1\n",
    "            totalDF[feat_idx] = boxcox1p(totalDF[feat_idx], self.lam)\n",
    "        return totalDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Normalization\n",
    "There is no need to define a specific class for data normalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimensionality Reduction\n",
    "I am thinking there is no need for dimensionality reduction at the moment. But I may change my mind later..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_folds = 5\n",
    "\n",
    "def rmsle_cv(model, train, y_train):\n",
    "    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(train.values)\n",
    "    rmse= np.sqrt(-cross_val_score(model, train.values, y_train, scoring=\"neg_mean_squared_error\", cv = kf))\n",
    "    return(rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training steps - models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC\n",
    "from sklearn.model_selection import KFold, cross_val_score, train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_file = 'house-prices-advanced-regression-techniques/train.csv'\n",
    "train_df = pd.read_csv(train_file)\n",
    "test_file = 'house-prices-advanced-regression-techniques/test.csv'\n",
    "test_df = pd.read_csv(test_file)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "descreteVars_Nominal = ['MSSubClass', 'MSZoning', 'Street', 'Alley', 'LandContour', 'LotConfig', 'Neighborhood',\n",
    "                        'Condition1', 'Condition2', 'BldgType', 'HouseStyle', 'RoofStyle', 'RoofMatl', 'Exterior1st',\n",
    "                        'Exterior2nd', 'MasVnrType', 'Foundation', 'Heating', 'CentralAir', 'Electrical',\n",
    "                        'GarageType', 'MiscFeature', 'SaleType', 'SaleCondition']\n",
    "\n",
    "descreteVars_Ordinal = ['LotShape', 'Utilities', 'LandSlope', 'ExterQual', 'ExterCond', 'BsmtQual',\n",
    "                        'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', 'HeatingQC',\n",
    "                        'KitchenQual','Functional',  'FireplaceQu', 'GarageFinish', 'GarageQual', 'GarageCond',\n",
    "                        'PavedDrive', 'PoolQC', 'Fence']\n",
    "\n",
    "resultVar = ['SalePrice']\n",
    "\n",
    "\n",
    "\n",
    "continuesVars = list(set(list(train_df)) - set(descreteVars_Nominal) - set(descreteVars_Ordinal) - set(resultVar))\n",
    "\n",
    "dict_address='cat_ord_dict.txt'\n",
    "\n",
    "\n",
    "X = train_df.drop(resultVar, axis=1)\n",
    "y = train_df[resultVar]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 3 required positional arguments: 'continuesVars', 'descreteVars_Ordinal', and 'descreteVars_Nominal'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-38-9a30cc540123>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0minputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdict_address\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontinuesVars\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdescreteVars_Ordinal\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdescreteVars_Nominal\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mlasso\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmake_pipeline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCatToNum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mCleanNum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mRobustScaler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mLasso\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0malpha\u001b[0m \u001b[1;33m=\u001b[0m\u001b[1;36m0.0005\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mlasso_transformed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmake_pipeline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCatToNum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdict_address\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontinuesVars\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdescreteVars_Ordinal\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdescreteVars_Nominal\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mCleanNum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDataTransformation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mRobustScaler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mLasso\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0malpha\u001b[0m \u001b[1;33m=\u001b[0m\u001b[1;36m0.0005\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# lasso = make_pipeline(CatToNum(dict_address, continuesVars, descreteVars_Ordinal, descreteVars_Nominal), CleanNum(), DataTransformation(), RobustScaler(), Lasso(alpha =0.0005, random_state=1))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: __init__() missing 3 required positional arguments: 'continuesVars', 'descreteVars_Ordinal', and 'descreteVars_Nominal'"
     ]
    }
   ],
   "source": [
    "lasso = make_pipeline(CatToNum(dict_address, continuesVars, descreteVars_Ordinal, descreteVars_Nominal), CleanNum(), RobustScaler(), Lasso(alpha =0.0005, random_state=1))\n",
    "lasso_transformed = make_pipeline(CatToNum(dict_address, continuesVars, descreteVars_Ordinal, descreteVars_Nominal), CleanNum(), DataTransformation(), RobustScaler(), Lasso(alpha =0.0005, random_state=1))\n",
    "\n",
    "# lasso = make_pipeline(CatToNum(dict_address, continuesVars, descreteVars_Ordinal, descreteVars_Nominal), CleanNum(), DataTransformation(), RobustScaler(), Lasso(alpha =0.0005, random_state=1))\n",
    "# lasso = make_pipeline(CatToNum(dict_address, continuesVars, descreteVars_Ordinal, descreteVars_Nominal), CleanNum(), DataTransformation(), RobustScaler(), Lasso(alpha =0.0005, random_state=1))\n",
    "# lasso = make_pipeline(CatToNum(dict_address, continuesVars, descreteVars_Ordinal, descreteVars_Nominal), CleanNum(), DataTransformation(), RobustScaler(), Lasso(alpha =0.0005, random_state=1))\n",
    "# lasso = make_pipeline(CatToNum(dict_address, continuesVars, descreteVars_Ordinal, descreteVars_Nominal), CleanNum(), DataTransformation(), RobustScaler(), Lasso(alpha =0.0005, random_state=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cal_rmse(model, X, y):\n",
    "    return (np.sqrt(mean_squared_error(y, model.predict(X))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "lasso_model_full_data = lasso.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32089.145583845155\n",
      "32960.93726922979\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "print(cal_rmse(lasso, X_train, y_train))\n",
    "print(cal_rmse(lasso, X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
